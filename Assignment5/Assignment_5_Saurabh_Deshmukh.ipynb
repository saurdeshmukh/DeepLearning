{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhdeshmukh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "dataX=np.load('ex5_train_x.npy')\n",
    "dataY=np.load('ex5_train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=h5py.File(\"Weights.hdf5\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):    \n",
    "    A = np.maximum(0,Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CVrelu(Z):    \n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_sigmoid(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    A1=sigmoid(Z)\n",
    "    cache = ((A, W, b), Z)\n",
    "    return A1, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_relu(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    A1=relu(Z)\n",
    "    cache = ((A, W, b), Z)\n",
    "    return A1, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward_sigmoid(dA, cache):\n",
    "    linear,Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    A_prev, W, b = linear\n",
    "    m = A_prev.shape[1]    \n",
    "    dW = (1/m) * np.dot(dZ, linear[0].T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(linear[1].T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_relu(dA,cache):\n",
    "    Z=cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward_relu(dA, cache):\n",
    "    linear,Z=cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    A_prev, W, b = linear\n",
    "    m = A_prev.shape[1]    \n",
    "    dW = (1/m) * np.dot(dZ, linear[0].T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(linear[1].T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_Encoding(Y,classes):\n",
    "    Yout=np.zeros((Y.shape[0],len(classes)))\n",
    "    for i in range(0,len(Y)-1):\n",
    "        indx=classes.index(Y[i])\n",
    "        Yout[i,indx]=1\n",
    "    return Yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(Inp, W, b):\n",
    "    s = np.multiply(Inp, W) + b\n",
    "    Z = np.sum(s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, parameters):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = parameters['W'].shape\n",
    "    stride = parameters['stride']\n",
    "    pad = parameters['pad']\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                \n",
    "        a_prev_pad = A_prev_pad[i]                     \n",
    "        for h in range(n_H):                          \n",
    "            for w in range(n_W):                      \n",
    "                for c in range(n_C):                   \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, parameters['W'][...,c], parameters['b'][...,c])\n",
    "    \n",
    "    cache = (A_prev,parameters)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    grads = {}\n",
    "    A_prev,parameters = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = parameters['W'].shape\n",
    "    stride = parameters[\"stride\"]\n",
    "    pad = parameters[\"pad\"]\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                      \n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += parameters['W'][:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "        if pad !=0:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[:,:,:]\n",
    "        grads[\"dW\"]=dW\n",
    "        grads[\"dA\"]=dA_prev\n",
    "        grads[\"db\"]=db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    for i in range(m):                           \n",
    "        for h in range(n_H):                     \n",
    "            for w in range(n_W):                \n",
    "                for c in range (n_C):            \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end  = horiz_start + f\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    cache = (A_prev, hparameters)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    mask = x == np.max(x)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_H * n_W)\n",
    "    a = np.ones(shape) * average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    grads = {}\n",
    "    (A_prev, hparameters) = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       \n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "    grads[\"dA\"]=dA_prev\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    Len = len(layers)\n",
    "    for i in range(1, Len):\n",
    "        parameters['W' + str(i)] = np.random.randn(layers[i], layers[i-1]) * 0.008\n",
    "        parameters['b' + str(i)] = np.random.randn(layers[i], 1)*0.008      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(A, Y,parameters=0,lambd=0):\n",
    "    m = Y.shape[1]\n",
    "    #print(m)\n",
    "    cost = None\n",
    "    #sumW=0\n",
    "    #Len = len(parameters) // 2\n",
    "    #for i in range(1,Len):\n",
    "    #    sumW=sumW+np.sum(np.square(parameters[\"W\" + str(i)]))\n",
    "     \n",
    "    cost = (-1/m)*np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y, np.log(1 - A))) #+ (lambd/(2*m))*sumW\n",
    "    cost = np.squeeze(cost) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescendent(FCparameters,conv1_param,conv2_param, gradsFC,gradsC1,gradsC2, lr,m=0,lambd=0):\n",
    "    L = len(FCparameters) // 2 \n",
    "    for l in range(L):\n",
    "        FCparameters[\"W\" + str(l+1)] = FCparameters[\"W\" + str(l+1)] - lr * gradsFC[\"dW\" + str(l+1)]#-((lr)/m)*FCparameters[\"W\" + str(l+1)]\n",
    "        FCparameters[\"b\" + str(l+1)] = FCparameters[\"b\" + str(l+1)] - lr * gradsFC[\"db\" + str(l+1)]\n",
    "    conv1_param['W']=conv1_param['W']-lr *gradsC1['dW']\n",
    "    conv1_param['b']=conv1_param['b']-lr *gradsC1['db']\n",
    "    conv2_param['W']=conv2_param['W']-lr *gradsC2['dW']\n",
    "    conv2_param['b']=conv2_param['b']-lr *gradsC2['db']\n",
    "    #return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_backpropogation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward_relu(dAL, current_cache)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_backward_sigmoid(grads[\"dA\" + str(l + 2)], current_cache)\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_forward_propogation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    Len = len(parameters) // 2                  \n",
    "    for i in range(1, Len):\n",
    "        A_prev = A \n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        A, cache = linear_forward_relu(A_prev, W, b)\n",
    "        caches.append(cache)\n",
    "    W = parameters[\"W\" + str(Len)]\n",
    "    b = parameters[\"b\" + str(Len)]\n",
    "    AL, cache = linear_forward_sigmoid(A, W, b)\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softMax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_initialize_parameters(prev_c,curr_c,filterSize,stride,pad):\n",
    "    parameters = {}\n",
    "    parameters['W'] = np.random.randn(filterSize[0],filterSize[1],prev_c,curr_c) * 0.008\n",
    "    parameters['b'] = np.random.randn(1,1,1,curr_c)*0.008    \n",
    "    parameters['stride'] = stride\n",
    "    parameters['pad'] = pad\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_initialize_parameters(stride,f):\n",
    "    parameters = {}   \n",
    "    parameters['stride'] = stride\n",
    "    parameters['f'] = f\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardAll(X,conv1_param,conv2_param,pool1_param,pool2_param,fc_params):\n",
    "    Z1,cache1 = conv_forward(X,conv1_param)\n",
    "    A1,cache2=CVrelu(Z1)\n",
    "    A2, cache3=pool_forward(A1,pool1_param)\n",
    "    Z2, cache4=conv_forward(A2,conv2_param)\n",
    "    A3,cache5=CVrelu(Z2)\n",
    "    A4,cache6=pool_forward(A3,pool2_param)\n",
    "    A4=A4.reshape(A4.shape[0],(A4.shape[1]*A4.shape[2]*A4.shape[3]))\n",
    "    A5,cache7=fc_forward_propogation(A4.T,fc_params)\n",
    "    out=softMax(A5)\n",
    "    return out,cache1,cache2,cache3,cache4,cache5,cache6,cache7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropogationAll(A, Y, cache1,cache2,cache3,cache4,cache5,cache6,cache7):\n",
    "    gradsFC=fc_backpropogation(A, Y,cache7)\n",
    "    dAfc=np.ndarray(gradsFC[\"dA1\"].shape)\n",
    "    dAfc=dAfc.T\n",
    "    dAfc=dAfc.reshape(dAfc.shape[0],9,9,16)\n",
    "    gradsPool2=pool_backward(dAfc, cache6)\n",
    "    #print(gradsPool2['dA'].shape)\n",
    "    dAPool2=gradsPool2['dA']\n",
    "    dZ1=backward_relu(dAPool2,cache5)\n",
    "    gradsC2=conv_backward(dZ1,cache4)\n",
    "    gradsPool1=pool_backward(gradsC2['dA'], cache3)\n",
    "    dAPool1=gradsPool1['dA']\n",
    "    dZ2=backward_relu(dAPool1,cache2)\n",
    "    gradsC1=conv_backward(dZ2,cache1)\n",
    "    return gradsFC,gradsPool2,gradsC2,gradsPool1,gradsC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, iteration,lr):\n",
    "    \n",
    "    conv1_param = conv_initialize_parameters(3,8,[4,4],2,1)\n",
    "    conv2_param = conv_initialize_parameters(8,16,[4,4],2,0)\n",
    "    pool1_param = pool_initialize_parameters(1,5)\n",
    "    pool2_param = pool_initialize_parameters(1,5)\n",
    "    ##Load weights from file\n",
    "    conv1_param['W']=np.array(dataset[\"c1W\"])\n",
    "    conv1_param['b']=np.array(dataset[\"c1b\"])\n",
    "    conv2_param['W']=np.array(dataset[\"c2W\"])\n",
    "    conv2_param['b']=np.array(dataset[\"c2b\"])\n",
    "    layers=[1296,108,6]\n",
    "    fc_params = fc_initialize_parameters(layers)\n",
    "    fc_params['W1']=np.array(dataset[\"FCw1\"])\n",
    "    fc_params['b1']=np.array(dataset[\"FCb1\"])\n",
    "    fc_params['W2']=np.array(dataset[\"FCw2\"])\n",
    "    fc_params['b2']=np.array(dataset[\"FCb2\"])\n",
    "    for i in range(0, iteration):\n",
    "        A,cache1,cache2,cache3,cache4,cache5,cache6,cache7=forwardAll(X,conv1_param,conv2_param,pool1_param,pool2_param,fc_params)\n",
    "        cost = cost_function(A, Y.T)\n",
    "        gradsFC,gradsPool2,gradsC2,gradsPool1,gradsC1 = backpropogationAll(A, Y.T, cache1,cache2,cache3,cache4,cache5,cache6,cache7)\n",
    "        gradientDescendent(fc_params,conv1_param,conv2_param,gradsFC,gradsC1,gradsC2, lr)\n",
    "        print (\"Cost : %f\" %(cost))\n",
    "    return conv1_param,conv2_param,fc_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 2.701776\n"
     ]
    }
   ],
   "source": [
    "Y=one_hot_Encoding(dataY,[0,1,2,3,4,5])\n",
    "X=(dataX/255)-0.5\n",
    "conv1_param,conv2_param,fc_params=model(X,Y,1,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
